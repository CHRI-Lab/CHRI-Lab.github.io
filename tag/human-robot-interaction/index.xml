<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>human-robot interaction | computational human-robot interaction lab</title>
    <link>https://CHRI-Lab.github.io/tag/human-robot-interaction/</link>
      <atom:link href="https://CHRI-Lab.github.io/tag/human-robot-interaction/index.xml" rel="self" type="application/rss+xml" />
    <description>human-robot interaction</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Research Group in HRI at CSE UNSW</copyright><lastBuildDate>Tue, 01 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://CHRI-Lab.github.io/images/icon_hu02c0217b7eeb0e7fdb8b09af6bff09b6_8240_512x512_fill_lanczos_center_2.png</url>
      <title>human-robot interaction</title>
      <link>https://CHRI-Lab.github.io/tag/human-robot-interaction/</link>
    </image>
    
    <item>
      <title>Augmented Robotics for Learners: A Case Study on Optics</title>
      <link>https://CHRI-Lab.github.io/publication/johal-augmented-2019/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/publication/johal-augmented-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Persuasive Robots - Exploring Behavioural Styles</title>
      <link>https://CHRI-Lab.github.io/prospective/undergrad/behavioural-styles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/prospective/undergrad/behavioural-styles/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;Social robots are foreseen to be encountered in our everyday life, playing roles as assistant or companion (to mention a few).
Recent studies have shown the potential harmful impacts of overtrust in social robotics [1], as robots may collect sensitive information without the user’s knowledge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://CHRI-Lab.github.io/img/student-projects/styles.png&#34; alt=&#34;graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;Behavioural styles  allow  robots to express themselves differently within the same context. Given a specific gesture, keyframe manipulation can be used in order to generate style-based variation to the gesture.
Behavioural styles have been studied in the past to improve robot&amp;rsquo;s behaviour during human-robot interaction [2].&lt;/p&gt;
&lt;p&gt;In this project, we will explore how behavioural styles can influence engagement, trust and persuasion during human-robot interaction.&lt;/p&gt;
&lt;h2 id=&#34;goals--milestones&#34;&gt;Goals &amp;amp; Milestones&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Implement behavioural styles for the Nao robot (voice and behaviour) and for a voice assistant (voice only)&lt;/li&gt;
&lt;li&gt;Design at least two behaviour styles based on human behaviour and personality styles&lt;/li&gt;
&lt;li&gt;Evaluate and compare these styles via experimentation&lt;/li&gt;
&lt;li&gt;Design a scenario similar to the one described in paper [3]&lt;/li&gt;
&lt;li&gt;Setup a data collection environment (posture, video and audio) in the HRI Lab facility of UNSW&lt;/li&gt;
&lt;li&gt;Select appropriate tasks and/or questionnaires to measure engagement, trust and/or persuasion&lt;/li&gt;
&lt;li&gt;Evaluate the system via an experiment with users&lt;/li&gt;
&lt;li&gt;Complete the data analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topics&#34;&gt;Topics&lt;/h2&gt;
&lt;p&gt;Robotics, HRI, Psychology&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Skills: Python, ROS and Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[1]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2019/10/14081257/Robots_social_impact_eng.pdf&lt;/li&gt;
&lt;li&gt;[2] &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.721.279&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johal, W., Pesty, S., &amp;amp; Calvary, G. (2014, August). Towards companion robots behaving with style. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication (pp. 1063-1068). IEEE.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] Bainbridge, W. A., Hart, J. W., Kim, E. S., &amp;amp; Scassellati, B. (2011). The benefits
of interactions with physically present robots over video-displayed agents.
International Journal of Social Robotics, 3(1), 41-52.&lt;/li&gt;
&lt;li&gt;[4] Peters, R., Broekens, J., Li, K., &amp;amp; Neerincx, M. A. (2019, July). Robot Dominance Expression Through Parameter-based Behaviour Modulation. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents (pp. 224-226). ACM.&lt;/li&gt;
&lt;li&gt;[5] Shane Saunderson et al. It Would Make Me Happy if You Used My Guess: Comparing Robot Persuasive Strategies in Social Human–Robot Interaction, IEEE Robotics and Automation Letters (2019). DOI: 10.1109/LRA.2019.2897143&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Rosona - Robot&#39;s Social Navigation</title>
      <link>https://CHRI-Lab.github.io/project/social_navigation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://CHRI-Lab.github.io/project/social_navigation/</guid>
      <description>&lt;h1 id=&#34;join-the-group&#34;&gt;Join the group&lt;/h1&gt;
&lt;h2 id=&#34;project-overview&#34;&gt;Project overview&lt;/h2&gt;
&lt;p&gt;Enabling robots to navigate in indoors environments in a safe and socially acceptable manner around groups of humans is still an open research area. Socially aware navigation considers the multi-modal assessment of the group dynamics, group formation inferring, path planning, real-time path adaptation, and human-robot communication.
Up to now the research in the field has considered a couple of classical scenarios such as crossing a group in a corridor or passing a door. Limitations in terms of lack of realistic datasets is often mentioned in the field. In this research project, we will aim to design several scenarios involving groups of humans in realistic settings. Our work will focus on three main tasks for the robot:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;approach and join a group, 2) passing by a group and 3) greeting a group. A first step of the project will be to record a novel dataset with rich interactions between the humans (H-H scenarios) and between humans and a teleoperated robot (H-R scenarios). The dataset will be collected at the HRI facility allowing multimodal synchronous recording. After that, a new model for path planning} will be developed. For the model, we will explore rule-based constraints (i.e. not passing between two persons speaking together) and learned constrained using the dataset recorded to infer implicit social norms. Finally, the model will be tested empirically with new users in which the robot will have to take real-time path planning decisions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;schedule&#34;&gt;Schedule&lt;/h2&gt;






  



  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://CHRI-Lab.github.io/project/social_navigation/schedule_huc88246b7b3883c0270f1e3cb63f3db19_136677_2000x2000_fit_lanczos_2.PNG&#34; &gt;


  &lt;img data-src=&#34;https://CHRI-Lab.github.io/project/social_navigation/schedule_huc88246b7b3883c0270f1e3cb63f3db19_136677_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1017&#34; height=&#34;631&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;expected-outcomes&#34;&gt;Expected Outcomes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A dataset featuring different scenarios of groups and spatial interactions will be recorded at the HRI facility in Paddington. After anonymization, the dataset will be made open.&lt;/li&gt;
&lt;li&gt;Development of a novel socially aware module allowing the robot to approach and leave groups using a hybrid method (rule-based and data-driven)
&lt;ul&gt;
&lt;li&gt;Empirical evaluation with end-users in the National Facility for HRI&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Report or conference publication at CoRL (Conference on Robot Learning – July 2021) or at the Robotics and Automation-Letters (RA-L)&lt;/li&gt;
&lt;li&gt;The code implemented during this project must be fully documented&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
