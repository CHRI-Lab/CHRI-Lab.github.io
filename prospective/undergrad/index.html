<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 5.0.0-beta.2 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Lina Phaijit">

  
  
  
    
  
  <meta name="description" content="Research group on Human-Robot Interaction in Australia">

  
  <link rel="alternate" hreflang="en-us" href="https://CHRI-Lab.github.io/prospective/undergrad/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#094183">
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous" media="print" onload="this.media='all'">

    
    
    
      
    
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Fira+Mono&family=Fira+Sans:wght@500&family=Source+Sans+Pro&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Fira+Mono&family=Fira+Sans:wght@500&family=Source+Sans+Pro&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.bcfb8fb76886b55618e485e65aa70adb.css">

  




  

  


  
  

  
  <link rel="alternate" href="/prospective/undergrad/index.xml" type="application/rss+xml" title="computational human-robot interaction lab">
  

  
  <link rel="manifest" href="/index.webmanifest">
  

  <link rel="icon" type="image/png" href="/images/icon_hu02c0217b7eeb0e7fdb8b09af6bff09b6_8240_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu02c0217b7eeb0e7fdb8b09af6bff09b6_8240_180x180_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://CHRI-Lab.github.io/prospective/undergrad/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="computational human-robot interaction lab">
  <meta property="og:url" content="https://CHRI-Lab.github.io/prospective/undergrad/">
  <meta property="og:title" content="Undergrads | computational human-robot interaction lab">
  <meta property="og:description" content="Research group on Human-Robot Interaction in Australia"><meta property="og:image" content="https://CHRI-Lab.github.io/images/icon_hu02c0217b7eeb0e7fdb8b09af6bff09b6_8240_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://CHRI-Lab.github.io/images/icon_hu02c0217b7eeb0e7fdb8b09af6bff09b6_8240_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
  

  




  


  





  <title>Undergrads | computational human-robot interaction lab</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  ">

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.7d03e63b5744a386cc8df717c3e2c75a.js"></script>

  



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">computational human-robot interaction lab</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">computational human-robot interaction lab</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        

        
        
        
        

        
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/project"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/publication"><span>Publications</span></a>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link  active" href="/prospective"><span>Prospective Collaborators</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      

      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Undergrads</h1>

  

  
</div>



<div class="universal-wrapper">
  
  <div class="article-style"><!---## Internships
Undergraduate students enrolled at Australian universities or the University of Auckland (New Zealand) can apply for [Taste of Research scholarships](https://www3.eng.unsw.edu.au/scholarships/tr2019/projects/school.cfm?id=4). --->
<h2 id="cse-honours-students">CSE Honours Students</h2>
<p>UNSW students that are enrolled (or would like to enrol) in the <a href="https://webcms3.cse.unsw.edu.au/THES0001/16s1/outline" target="_blank" rel="noopener">Computer Science (Honours) program</a> at UNSW can see my proposed honours thesis topics in the CSE Thesis Topics Database or looked at the proposed project with the tag &ldquo;cse-honours&rdquo;.</p>
<h3 id="list-of-currently-offered-projects-for-cse-honours-students">List of currently offered Projects for CSE Honours students</h3>
</div>
  

  
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/e-pen/" >E-Pen</a></h2>
      <div class="article-style">
        
          Context Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school. It is even more true for people having to learn different handwriting scripts (i.e. latin, chinese, arabic)
In this project we propose to explore new methods to assess and train people&rsquo;s handwriting in a multiscript handwriting application. The project will aim to develop a new engaging handwriting analysis tool and integrate the analysis in agamified application. The backend of the application will be performing the analsysi of handwirting through a library taking into account various features of the handwriting logs (i.e. pen pressure, tilt, speed).
Goals &amp; Milestones During this project, the student will:
 Develop a library able to analyse strokes and handwriting (backend) Develop a JS app able to record handwriting data Integrate gamification into the app to build a learning game Evaluate the implemented application with end-users evaluating both usability and performances (learning outcomes)  Topics Handwriting, JS, Algoritms,
Prerequisites  Skills: JS, Python, Git.  References See Collection https://wafa.johal.org/tags/handwriting/
        
      </div>
    </div>
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/fyv_2020/" >Find Your Voice: Use of Voice Assistant for Learning</a></h2>
      <div class="article-style">
        
          Context Many children struggle to find their voice in social situations. They may be shy, suffer from social anxiety, be new to a culture (migrants) or have impairments in communication due to atypical development (e.g., autism spectrum disorder, speech or hearing disorders).
The voices of these children often go unheard, as they find it hard to contribute to a conversation.
The Find your Voice (FyV, http://wafa.johal.org/project/fyv/) project was initiated to investigate how joke telling could help children to speak up and gain confidence. We are also interested in story telling and general conversation. Improvements in communication can have a significant impact in confidence, and help children:
 reduce stress improve self-confidence ease social interactions make friends more easily improving literacy and language  To help children develop the ability to communicate, tell jokes or stories to their peers, we propose leveraging social robots (e.g. NAO) and voice assistants (e.g., Alexa, Olly and Google Home) to:
 Model how to tell jokes/stories and respond to other children during conversations . Practice joke/story telling with a ‘friendly’ and ‘non-judgmental’ audience. Practice turn taking during conversation. TLearn jokes, stories and interesting facts to tell other children.  The overall goals of the project are:
 To enable children to improve their social communication skills by learning intonation and timing, through interacting with voice assistants To learn to how to perform in front of peers and family To make children more confident in social situations  The FyV project involves partners in London and California.
Goals &amp; Milestones At UNSW, our main goal will be to develop a ‘Learning by Teaching’ application using a robot or voice assistant. This application will allow the user to teach a virtual agent (robot or voice assistant) a joke/story. As the agent learns by demonstration, the user can practice and refine how the story/joke is told until the voice assistant (and the child) is able to tell the joke/story in a satisfactory way.
        
      </div>
    </div>
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/human_action_recognition/" >Human Action Recognition from AD Movies</a></h2>
      <div class="article-style">
        
          Context Action Recognition is curcial for robots to perfoma around humans. Indeed, robot need to asses human action and intentions in order to assist them in everyday life tasks and to collaborative efficiently.
The field of action recognition has aimed to use typical sensors found on robots to recognize agnts, objects and actions they are performing. Typical approach is to record a dataset of various action and label them. But often theses actions are not natural and it can be difficult to represent the variety of ways to perform actions with a lab built dataset. In this project we propose to use audio desription movies to label actions. AD movies integrate a form of narration to allow virually impaired veiwers to undertsnad the visual element sowed on screen. This information often deals with action actually depicted on the scene.
Goals &amp; Milestones During this project, the student will:
 Develop a pipeline to collect and crop clip of AD movies for at home actions. This extraction tool should be fexible and allow for integration of next action. It will for instance feature video and text processing to extract [Subject+ Action + Object] type of data. Investigate methods for HAR Implement a tree model combaning HAR with YOLO to identify agent and objects Evaluate the HAR pipeline with the Toyota Robot HSR  Topics Human Action Recognition,
Prerequisites  Skills: Python, C++, Git.  References  https://www-sciencedirect-com.wwwproxy1.library.unsw.edu.au/science/article/pii/S174228761930283X https://openaccess.thecvf.com/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf https://dl-acm-org.wwwproxy1.library.unsw.edu.au/doi/abs/10.1145/3355390?casa_token=MrZSE8hoPFYAAAAA:rcwHYdISRyLM5OApuN_2SASbwgBsswxx2EPHy9mGP8NaqIdvBj0q5LIa9_ChdyI_Lzfi4GX0PWjhD54 https://prior.allenai.org/projects/charades https://arxiv.org/pdf/1708.02696.pdf https://arxiv.org/pdf/1806.11230.pdf  
        
      </div>
    </div>
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/hri-dataset-mine/" >Machine Learning for Social Interaction Modelling</a></h2>
      <div class="article-style">
        
          Context The field of social human-robot interaction is growing. Understanding how communication between humans (human-human) generalises during human-robot communication is crucial in building fluent and enjoyable interactions with social robots. Everyday, more datasets that feature social interaction between humans and between humans and robots are made freely available online.
In this project we propose to take a data-driven approach to build predictive models of social interactions between humans (HH) ( and between humans and robots (HR) interaction using 3 different datasets. Relevant research questions include:
 Which multi-modal features can be transferable from HH to HR setups? Are there common features that discriminate human behaviour in HH or HR scenarios (e.g. &lsquo;Do people speak less or slower with robots?&rsquo; &hellip; )  Goals &amp; Milestones During this project, the student will:
 Explore datasets (PinSoRo, MHHRI and P2PSTORY): type of data (video; audio, point cloud), available labels and annotation &hellip; Extract relevant features multimodal on each dataset Evaluate predictive models for each dataset (i.e. engagement) Explore transfer learning from one dataset to another  There is also potential to use UNSW’s National Facility for Human-Robot Interaction Research to create a new dataset.
Topics Machine Learning, Human-Robot Interaction
Prerequisites  Skills: Python, ROS, Git.  References  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205999 https://www.cl.cam.ac.uk/research/rainbow/projects/mhhri/ https://www.media.mit.edu/projects/p2pstory/overview/  
        
      </div>
    </div>
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/behavioural-styles/" >Persuasive Robots - Exploring Behavioural Styles</a></h2>
      <div class="article-style">
        
          Context Social robots are foreseen to be encountered in our everyday life, playing roles as assistant or companion (to mention a few). Recent studies have shown the potential harmful impacts of overtrust in social robotics [1], as robots may collect sensitive information without the user’s knowledge.
Behavioural styles allow robots to express themselves differently within the same context. Given a specific gesture, keyframe manipulation can be used in order to generate style-based variation to the gesture. Behavioural styles have been studied in the past to improve robot&rsquo;s behaviour during human-robot interaction [2].
In this project, we will explore how behavioural styles can influence engagement, trust and persuasion during human-robot interaction.
Goals &amp; Milestones  Implement behavioural styles for the Nao robot (voice and behaviour) and for a voice assistant (voice only) Design at least two behaviour styles based on human behaviour and personality styles Evaluate and compare these styles via experimentation Design a scenario similar to the one described in paper [3] Setup a data collection environment (posture, video and audio) in the HRI Lab facility of UNSW Select appropriate tasks and/or questionnaires to measure engagement, trust and/or persuasion Evaluate the system via an experiment with users Complete the data analysis  Topics Robotics, HRI, Psychology
Prerequisites  Skills: Python, ROS and Git.  References  [1]https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2019/10/14081257/Robots_social_impact_eng.pdf [2] Johal, W., Pesty, S., &amp; Calvary, G. (2014, August). Towards companion robots behaving with style. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication (pp. 1063-1068). IEEE. [3] Bainbridge, W. A., Hart, J. W., Kim, E. S., &amp; Scassellati, B. (2011). The benefits of interactions with physically present robots over video-displayed agents. International Journal of Social Robotics, 3(1), 41-52. [4] Peters, R., Broekens, J., Li, K., &amp; Neerincx, M. A. (2019, July). Robot Dominance Expression Through Parameter-based Behaviour Modulation. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents (pp.
        
      </div>
    </div>
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/writing_2020/" >Robot Writing</a></h2>
      <div class="article-style">
        
          Context Handwriting is one of the most important motor skill we learn as human. Children with handwriting difficulties can find their academic impacted and often succeed less at school.
In this project we propose to explore new methods for a robot to write using a pen. The project will use the Fetch Robot and aim to evaluate how a robot could learn handwriting trajectories using human demonstrations.
Goals &amp; Milestones During this project, the student will:
 Learn about ROS Develop a ROS package that enables to control the Fetch Robot arm given a series of handwriting strokes Explore the use of different methods for the robot to learn letter writing from demonstrations Evaluate the implemented method compare to other state of the art methods  Topics ROS, Learning by Demonstration, Robotics, Handwriting
Prerequisites  Skills: Python, C++, ROS, Git.  References See Zotero Collection https://www.zotero.org/groups/2419050/hri-unsw/collections/GQERYTFZ
        
      </div>
    </div>
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/tip_2020/" >Tangible e-Ink Paper Interfaces for Learners</a></h2>
      <div class="article-style">
        
          Context While digital tools are more and more used in classrooms, teachers' common practice remains to use photocopied paper documents to share and collect learning exercises from their students. With the Tangible e-Ink Paper (TIP) system, we aim to explore the use of tangible manipulatives interacting with paper sheets as a bridge between digital and paper traces of learning. Featuring an e-Ink display, a paper-based localisation system and a wireless connection, TIPs are envisioned to be used as a versatile tool across various curriculum activities.
Goals &amp; Milestones  Literature Review on Tangible UI (TUI) in Education Implement and test a proof of concept of TIPs for learning Assemble 3 TIPs (3D printing of parts, soldering, etc.) Install libraries on Rasberry PI ( e.g. libdots - used for self-paper-based localisation, bluetooth communication) Develop two demo applications using TIPs for  individual work (on A4 sheet of paper) collaborative work (on min A2)    Topics Tangible User Interfaces, HCI
Prerequisites and Learning Outcomes  Skills: Javascript, Python or C++. Git. Qt, Rasberry Pi  References  https://infoscience.epfl.ch/record/271833/files/paper.pdf https://infoscience.epfl.ch/record/224129/files/paper.pdf https://infoscience.epfl.ch/record/270077/files/Robot_Analytics.pdf https://link.springer.com/chapter/10.1007/978-3-030-29736-7_57  
        
      </div>
    </div>
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/h-swarm_2020/" >Tangible Human Swarm Interaction</a></h2>
      <div class="article-style">
        
          Context: Visuo-Motor coordination problems can impair children in their academic achievements and in their everyday life. Gross visuo-motor skills, in particular, are required in a range of social and educational activities that contribute to children&rsquo;s physical and cognitive development such as playing a musical instrument, ball-based sports or dancing. Children with visuo-motor coordination difficulties are typically diagnosed with developmental coordination disorder or cerebral palsy and need undergo physical therapy. The therapy sessions are often not engaging for children and conducted individually. In this project, we aim to design new forms of interaction with a swarm for enhance visuo-motor coordination. We propose to develop a game that allows multiple children to play collaboratively on the same table.
Goals &amp; Milestones  Implement the set of basic swarm behaviour using 4 Cellulo robots Integrate collaorative and tangible interactions Test the system with a participants. We plan ti integrate a measure of cognitive load using eye tracking data  Topics HCI, Health, Game, Swarm Robotics
Prerequisites  Skills: Python, C++, Js  References  https://www.epfl.ch/labs/chili/index-html/research/cellulo/  
        
      </div>
    </div>
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/tangible_online/" >Tangible Robots for Collaborative Online Learning</a></h2>
      <div class="article-style">
        
          Context: Online learning presents several advantages: decreasing cost, allowing more flexibility and access to far away training resources. However, studies have found that it also limits communications between peers and teachers, limits physical interactions and that it requires a big commitment on the student&rsquo;s part to plan and stay assiduous in their learning.
Goals &amp; Milestones In this project, we aim to design and test a novel way to engage students in collaborative online learning by using haptic enabled tangible robots. The project will consist in:
 developing a tool allowing the design of online activities for two or more robots to be connected implementing a demonstrator for this new library that will embed a series of small exercises hilightling the new capability of remote haptic-assisted collaboration evaluating the demonstrator with a user experiment  Topics HCI, Haptics, Robot, Collaborative Work (Training/Gaminig)
Prerequisites  Skills: C++, Js,  References See Zotero Collection https://www.zotero.org/groups/2419050/hri-unsw/collections/JXBHFMBC
 Schneider, B., Jermann, P., Zufferey, G., &amp; Dillenbourg, P. (2011). Benefits of a Tangible Interface for Collaborative Learning and Interaction. IEEE Transactions on Learning Technologies, 4(3), 222–232. https://doi.org/10.1109/TLT.2010.36 Asselborn, T., Guneysu, A., Mrini, K., Yadollahi, E., Ozgur, A., Johal, W., &amp; Dillenbourg, P. (2018). Bringing letters to life: Handwriting with haptic-enabled tangible robots. Proceedings of the 17th ACM Conference on Interaction Design and Children, 219–230. East, B., DeLong, S., Manshaei, R., Arif, A., &amp; Mazalek, A. (2016). Actibles: Open Source Active Tangibles. Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces, 469–472. https://doi.org/10.1145/2992154.2996874 Guinness, D., Muehlbradt, A., Szafir, D., &amp; Kane, S. K. (2019a). RoboGraphics: Dynamic Tactile Graphics Powered by Mobile Robots. The 21st International ACM SIGACCESS Conference on Computers and Accessibility, 318–328. https://doi.org/10.1145/3308561.3353804 Guinness, D., Muehlbradt, A., Szafir, D., &amp; Kane, S. K. (2019b). RoboGraphics: Using Mobile Robots to Create Dynamic Tactile Graphics.
        
      </div>
    </div>
  
    
    
    
    <div>
      <h2><a href="/prospective/undergrad/voice-robot/" >Voice for ROS</a></h2>
      <div class="article-style">
        
          Context Natural language is an important part of communication since it offers an intuitive and efficient way of conveying ideas to another individual. Enabling robots to efficiently use language is essential for human-robot collaboration. In this project, we aim to develop an interface between a dialog manager (i.e. DialogFlow) and ROS (Robotics Operating System). By doing this, we will be able to use the powerful dialogue systems in human-robot interaction scenario.
A scenario, using tangible robots (Cellulo) combined with voice assistant for upper-arm rehabilitation will be implemented to show the potential of this new ros-package.
Goals &amp; Milestones During this project, the student will:
 Learn about Google DialogFlow and ROS Develop a ROS package that enables to access and manipulates DialogFlow features Develop a Cellulo Rehabilitation Game Test the game with a pilot experiment  Topics Voice-Assistant, Human-Robot Interaction, ROS
Prerequisites  Skills: Python, C++, ROS, Git.  References  https://dialogflow.com/ https://www.ros.org/ http://wafa.johal.org/project/cellulo/ Hudson, C., Bethel, C. L., Carruth, D. W., Pleva, M., Juhar, J., &amp; Ondas, S. (2017, October). A training tool for speech driven human-robot interaction applications. In 2017 15th International Conference on Emerging eLearning Technologies and Applications (ICETA) (pp. 1-6). IEEE. Moore, R. K. (2017). Is spoken language all-or-nothing? Implications for future speech-based human-machine interaction. In Dialogues with Social Robots (pp. 281-291). Springer, Singapore. Beirl, D., Yuill, N., &amp; Rogers, Y. (2019). Using Voice Assistant Skills in Family Life. In Lund, K., Niccolai, G. P., Lavoué, E., Gweon, C. H., &amp; Baker, M. (Eds.), A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings, 13th International Conference on Computer Supported Collaborative Learning (CSCL) 2019, Volume 1 (pp. 96-103). Lyon, France: International Society of the Learning Sciences.  
        
      </div>
    </div>
  

  

</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">
  

  <p class="powered-by">
    © Research Group in HRI in Australia (UoM, UNSW)
  </p>

  
  






  <p class="powered-by">
    
    
    
    Published with
    <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    

    
    

    
    
    

    
    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    <script src="/en/js/wowchemy.min.2482b05220ab5a1288060cb689889796.js"></script>

    






</body>
</html>
